{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_recognition_deep_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYoti26qtlp8SVasI06+S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindy-cyber/image_recognition/blob/main/image_recognition_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQzLQI7N0Pcp"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras #add layers\n",
        "import matplotlib.pyplot as plt #graphing library"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47AmKXMv2Bqh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "7f351c91-1010-41e8-caf2-f0430bace74c"
      },
      "source": [
        "########data processing phase\n",
        "\n",
        "#load dataset\n",
        "data = keras.datasets.fashion_mnist #instead of loading a csv\n",
        "\n",
        "#split data\n",
        "(train_images, train_labels),(test_images,test_labels) = data.load_data()\n",
        "\n",
        "#observe data\n",
        "print(train_images.shape)\n",
        "\n",
        "# the labels are just numbers.\n",
        "# to make our lives easier, lets map the numbers to class names \n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "index = 9\n",
        "image = train_images[index] #0-5999\n",
        "label = train_labels[index]\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(class_names[label]) #all the labels are numbers, we need to name the bucket\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLxGbogn516z"
      },
      "source": [
        "#keras API expexts 4 dimensions, need to reshape\n",
        "#(60000, 28,28, 1) since we dont have rgb or stuff\n",
        "train_images = train_images.reshape((train_images.shape[0],28,28,1)) #all images reshape to 28,28 -> 28,28,1, leave the 60000 alone\n",
        "test_images =  test_images.reshape((test_images.shape[0],28,28,1))\n",
        "\n",
        "#plt.imshow(image)\n",
        "#plt.title(class_names[label])\n",
        "#plt.show()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF3QwzXH7dA1"
      },
      "source": [
        "#shrink our dataset, from 0,255 to 0,1(normalize)\n",
        "train_images = train_images/255.0 #(numpy arrays does it to every entry, brightness is the only factor here) \n",
        "test_images =  test_images/255.0  \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mfgc-DZDdA8"
      },
      "source": [
        "[ [255, 128, … 128, 0]] [ [1.0, 0.5, …. 0.5, 0] ], the brightness of everything\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSIKRSJ8c6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87db0b1f-f798-474c-f10d-1d5b5be0ce89"
      },
      "source": [
        "#stacking the layers\n",
        "model = keras.Sequential()\n",
        "#input layer flatten from 2D list to 1D\n",
        "model.add(keras.layers.Conv2D(28, (8,8), padding = \"same\", activation = \"relu\"))   \n",
        "\n",
        "\n",
        "#model.add(keras.layers.Conv2D(1, (18,18), padding = \"same\", activation = \"relu\"))#adding two many conv2D doesnt help with anything but slows down the speed\n",
        "#model.add(keras.layers.GlobalMaxPooling2D()) #generate a value that will summarize the strongest activation or presence of the vertical line in the input image.\n",
        "model.add(keras.layers.Flatten(input_shape = (28,28,1))) #first layer\n",
        "\n",
        "model.add(keras.layers.Dense(50, activation = \"relu\"))\n",
        "\n",
        "#continue to add layers\n",
        "#use 10 different filters, each with the size\n",
        "#keras.layers.Flatten()\n",
        "#output\n",
        "#10 classes -> 10 neurons, softmax(like sigmoid, probablity) for polynomial classification\n",
        "model.add(keras.layers.Dense(10,activation = \"softmax\"))\n",
        "\n",
        "#compile the model\n",
        "#loss: categorical, how to calculate the loss function cuz we have multiple buckets\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#train the model\n",
        "model.fit(train_images, train_labels, epochs = 10)\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.5206 - accuracy: 0.8130\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.2838 - accuracy: 0.8960\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.2295 - accuracy: 0.9155\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1940 - accuracy: 0.9297\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1667 - accuracy: 0.9375\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1379 - accuracy: 0.9485\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1135 - accuracy: 0.9571\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 67s 36ms/step - loss: 0.0944 - accuracy: 0.9654\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 67s 36ms/step - loss: 0.0804 - accuracy: 0.9706\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.0652 - accuracy: 0.9764\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (32, 28, 28, 28)          1820      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (32, 21952)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (32, 50)                  1097650   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (32, 10)                  510       \n",
            "=================================================================\n",
            "Total params: 1,099,980\n",
            "Trainable params: 1,099,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVy0a6ukEDXt"
      },
      "source": [
        "12730 = 12704 + 10 + 26"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAQC8gJJEwom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec398a5a-7a6c-4590-9789-d75015b13b4e"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_images,test_labels)\n",
        "print(test_loss, test_accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 13ms/step - loss: 0.4044 - accuracy: 0.9018\n",
            "0.40441226959228516 0.9017999768257141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GGHFubLGOzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f390d196-4ecf-46f3-fee9-f7b9f47e827f"
      },
      "source": [
        "#predict\n",
        "import numpy as np\n",
        "predictions = model.predict(test_images)\n",
        "index = 5\n",
        "#the list of confidence levels\n",
        "print(predictions[index])\n",
        "predicted_label_index = np.argmax(predictions[index])\n",
        "predicted_class_label =  class_names[predicted_label_index]\n",
        "actual_label_index = test_labels[index]\n",
        "actual_class_label = class_names[actual_label_index]\n",
        "print(predicted_class_label, actual_class_label)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.7903794e-16 1.0000000e+00 1.9999578e-24 7.2407510e-15 1.9811547e-26\n",
            " 8.8710532e-21 1.7080889e-20 2.5811032e-20 1.7403781e-18 3.7044681e-23]\n",
            "Trouser Trouser\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI8ytOe0QRB0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}